{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 9130570,
          "sourceType": "datasetVersion",
          "datasetId": 5512695
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Local TTS",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/G0-h-g/Project_1/blob/main/Local_TTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.1.2 numpy==1.23.5 transformers==4.38.2 uroman-python==1.2.8.1 datasets==2.16.1 deepfilternet==0.5.6 torchaudio==2.1.2 librosa==0.10.0 pydub==0.25.1 speechbrain==0.5.16"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "cxHyVVbadHy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import os\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "speaker_model = EncoderClassifier.from_hparams(\n",
        "    source=spk_model_name,\n",
        "    run_opts={\"device\": device},\n",
        "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
        ")\n",
        "\n",
        "def create_speaker_embedding(waveform):\n",
        "    with torch.no_grad():\n",
        "        speaker_embeddings = speaker_model.encode_batch(waveform)\n",
        "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=-1)\n",
        "    return speaker_embeddings"
      ],
      "metadata": {
        "trusted": true,
        "id": "a_27X5kSdHy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import requests\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "# from src.reduce_noise import smooth_and_reduce_noise, model_remove_noise, model, df_state\n",
        "import io\n",
        "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
        "from pydub import AudioSegment\n",
        "import re\n",
        "from uroman import uroman\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"truong-xuan-linh/vi-xvector-speechbrain\",\n",
        "                       download_mode=\"force_redownload\",\n",
        "                            verification_mode=\"no_checks\",\n",
        "                            cache_dir=\"temp/\",\n",
        "                            revision=\"5ea5e4345258333cbc6d1dd2544f6c658e66a634\")\n",
        "dataset = dataset[\"train\"].to_list()\n",
        "\n",
        "dataset_dict = {}\n",
        "\n",
        "for rc in dataset:\n",
        "    dataset_dict[rc[\"speaker_id\"]] = rc[\"embedding\"]\n",
        "\n",
        "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
        "\n",
        "def remove_special_characters(sentence):\n",
        "    # Use regular expression to keep only letters, periods, and commas\n",
        "    sentence_after_removal =  re.sub(r'[^a-zA-Z\\s,.\\u00C0-\\u1EF9]', ' ,', sentence)\n",
        "    return sentence_after_removal\n",
        "\n",
        "from scipy.signal import butter, lfilter\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "def korean_splitter(string):\n",
        "    pattern = re.compile('[가-힣]+')\n",
        "    matches = pattern.findall(string)\n",
        "    return matches\n",
        "\n",
        "def uroman_normalization(string):\n",
        "    korean_inputs = korean_splitter(string)\n",
        "    for korean_input in korean_inputs:\n",
        "        korean_roman = uroman(korean_input)\n",
        "        string = string.replace(korean_input, korean_roman)\n",
        "    return string\n",
        "\n",
        "class Model():\n",
        "\n",
        "    def __init__(self, model_name, speaker_url=\"\"):\n",
        "        self.model_name = model_name\n",
        "        self.processor = SpeechT5Processor.from_pretrained(model_name)\n",
        "        self.model = SpeechT5ForTextToSpeech.from_pretrained(model_name)\n",
        "        # self.model.generate = partial(self.model.generate, use_cache=True)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        self.speaker_url = speaker_url\n",
        "        if speaker_url:\n",
        "\n",
        "            print(f\"download speaker_url\")\n",
        "            response = requests.get(speaker_url)\n",
        "            audio_stream = io.BytesIO(response.content)\n",
        "            audio_segment = AudioSegment.from_file(audio_stream, format=\"wav\")\n",
        "            audio_segment = audio_segment.set_channels(1)\n",
        "            audio_segment = audio_segment.set_frame_rate(16000)\n",
        "            audio_segment = audio_segment.set_sample_width(2)\n",
        "            wavform, _ = torchaudio.load(audio_segment.export())\n",
        "            self.speaker_embeddings = create_speaker_embedding(wavform)[0]\n",
        "        else:\n",
        "            self.speaker_embeddings = None\n",
        "\n",
        "        if model_name == \"truong-xuan-linh/speecht5-vietnamese-commonvoice\" or model_name == \"truong-xuan-linh/speecht5-irmvivoice\":\n",
        "            self.speaker_embeddings = torch.zeros((1, 512))  # or load xvectors from a file\n",
        "\n",
        "    def inference(self, text, speaker_id=None):\n",
        "        # if self.model_name == \"truong-xuan-linh/speecht5-vietnamese-voiceclone-v2\":\n",
        "        #     # self.speaker_embeddings = torch.tensor(dataset_dict_v2[speaker_id])\n",
        "        #     wavform, _ = torchaudio.load(speaker_id)\n",
        "        #     self.speaker_embeddings = create_speaker_embedding(wavform)[0]\n",
        "\n",
        "        if \"voiceclone\" in self.model_name:\n",
        "            if not self.speaker_url:\n",
        "                self.speaker_embeddings = torch.tensor(dataset_dict[speaker_id])\n",
        "            # self.speaker_embeddings = create_speaker_embedding(speaker_id)[0]\n",
        "            # wavform, _ = torchaudio.load(\"voices/kcbn1.wav\")\n",
        "            # self.speaker_embeddings = create_speaker_embedding(wavform)[0]\n",
        "            # wavform, _ = torchaudio.load(wav_file)\n",
        "            # self.speaker_embeddings = create_speaker_embedding(wavform)[0]\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            full_speech = []\n",
        "            separators = r\";|\\.|!|\\?|\\n\"\n",
        "            text = uroman_normalization(text)\n",
        "            text = remove_special_characters(text)\n",
        "            text = text.replace(\" \", \"▁\")\n",
        "            split_texts = re.split(separators, text)\n",
        "\n",
        "            for split_text in split_texts:\n",
        "\n",
        "                if split_text != \"▁\":\n",
        "                    split_text = split_text.lower() + \"▁\"\n",
        "                    print(split_text)\n",
        "                    inputs = self.processor.tokenizer(text=split_text, return_tensors=\"pt\")\n",
        "                    speech = self.model.generate_speech(inputs[\"input_ids\"], threshold=0.5, speaker_embeddings=self.speaker_embeddings, vocoder=vocoder)\n",
        "                    full_speech.append(speech.numpy())\n",
        "                    # full_speech.append(butter_bandpass_filter(speech.numpy(), lowcut=10, highcut=5000, fs=16000, order=2))\n",
        "            # out_audio = model_remove_noise(model, df_state, np.concatenate(full_speech))\n",
        "            return np.concatenate(full_speech)\n",
        "\n",
        "    @staticmethod\n",
        "    def moving_average(data, window_size):\n",
        "        return np.convolve(data, np.ones(window_size)/window_size, mode='same')"
      ],
      "metadata": {
        "trusted": true,
        "id": "zW-n8YKmdHy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-08T09:00:14.166253Z",
          "iopub.execute_input": "2024-08-08T09:00:14.1667Z",
          "iopub.status.idle": "2024-08-08T09:00:51.44335Z",
          "shell.execute_reply.started": "2024-08-08T09:00:14.166663Z",
          "shell.execute_reply": "2024-08-08T09:00:51.441844Z"
        },
        "trusted": true,
        "id": "E6b-RtTMdHy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Khởi tạo mô hình\n",
        "model = Model(\n",
        "    model_name=\"truong-xuan-linh/speecht5-vietnamese-voiceclone-lsvsc\",\n",
        "    speaker_url=\"\"\n",
        ")\n",
        "\n",
        "speaker_id = \"speech_dataset_denoised\"\n",
        "\n",
        "def read_srt(file_path):\n",
        "    subtitles = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        for i in range(0, len(lines), 4):\n",
        "            if i+2 < len(lines):\n",
        "                start_time, end_time = lines[i+1].strip().split(' --> ')\n",
        "                text = lines[i+2].strip()\n",
        "                subtitles.append((start_time, end_time, text))\n",
        "\n",
        "    return subtitles\n",
        "\n",
        "def time_to_seconds(time_str):\n",
        "    h, m, s = time_str.split(':')\n",
        "    seconds = int(h) * 3600 + int(m) * 60 + float(s.replace(',', '.'))\n",
        "    return seconds\n",
        "\n",
        "def generate_audio_with_pause(srt_file):\n",
        "    subtitles = read_srt(srt_file)\n",
        "    audio_clips = []\n",
        "\n",
        "    for i, (start_time, end_time, text) in enumerate(subtitles):\n",
        "        audio_data = model.inference(text=text, speaker_id=speaker_id)\n",
        "        audio_data = audio_data / np.max(np.abs(audio_data))  # Chuẩn hóa dữ liệu âm thanh\n",
        "\n",
        "        audio_clips.append(audio_data)\n",
        "\n",
        "        # Thêm khoảng thời gian nghỉ\n",
        "        if i < len(subtitles) - 1:\n",
        "            next_start_time = subtitles[i + 1][0]\n",
        "            pause_duration = time_to_seconds(next_start_time) - time_to_seconds(end_time)\n",
        "            if pause_duration > 0:\n",
        "                pause_samples = int(pause_duration * 16000)\n",
        "                audio_clips.append(np.zeros(pause_samples))\n",
        "\n",
        "    # Kết hợp tất cả các đoạn âm thanh\n",
        "    final_audio = np.concatenate(audio_clips)\n",
        "\n",
        "    # Trả về đối tượng âm thanh\n",
        "    return Audio(final_audio, rate=16000)\n",
        "\n",
        "# Ví dụ sử dụng\n",
        "srt_file_path = '/kaggle/input/file-srt/test.srt'\n",
        "audio = generate_audio_with_pause(srt_file_path)\n",
        "# audio.display()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-08T09:07:57.272832Z",
          "iopub.execute_input": "2024-08-08T09:07:57.273328Z",
          "iopub.status.idle": "2024-08-08T09:08:07.885718Z",
          "shell.execute_reply.started": "2024-08-08T09:07:57.27329Z",
          "shell.execute_reply": "2024-08-08T09:08:07.883557Z"
        },
        "trusted": true,
        "id": "ywoHVvf_dHy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-08T09:08:09.404993Z",
          "iopub.execute_input": "2024-08-08T09:08:09.40587Z",
          "iopub.status.idle": "2024-08-08T09:08:09.423584Z",
          "shell.execute_reply.started": "2024-08-08T09:08:09.405829Z",
          "shell.execute_reply": "2024-08-08T09:08:09.422364Z"
        },
        "trusted": true,
        "id": "EwS2MLhFdHy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}